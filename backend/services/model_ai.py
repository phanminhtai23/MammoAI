import os
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import requests
import tempfile
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from pathlib import Path
import asyncio
import aiohttp
import gc  # Garbage collection ƒë·ªÉ gi·∫£i ph√≥ng RAM
from database import models_collection


class ModelAI:
    def __init__(self):
        """
        Kh·ªüi t·∫°o ModelAI class cho PyTorch models (Docker-friendly)
        """
        self.current_model = None
        self.model_info = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.class_names = ['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5']
        
        # Docker-friendly cache directory
        cache_base = os.getenv("MODEL_CACHE_DIR", "/app/model_cache")
        self.model_cache_dir = Path(cache_base)
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        self.current_cache_file = None  # Track current cached file
        
        # Transform cho ·∫£nh (ImageNet pretrained)
        self.imagenet_mean = [0.485, 0.456, 0.406]
        self.imagenet_std = [0.229, 0.224, 0.225]
        self.size = (299, 299)  
        
        self.transform = transforms.Compose([
            transforms.Resize(self.size),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std),
        ])
        
        print(f"üê≥ ModelAI initialized - Cache dir: {self.model_cache_dir}")
        print(f"üéØ Device: {self.device}")
    
    def _clear_old_model_from_memory(self):
        """
        X√≥a model c≈© kh·ªèi RAM v√† gi·∫£i ph√≥ng memory (Docker-optimized)
        """
        if self.current_model is not None:
            # Move model to CPU tr∆∞·ªõc khi x√≥a (gi·∫£i ph√≥ng VRAM n·∫øu d√πng GPU)
            if hasattr(self.current_model, 'cpu'):
                self.current_model.cpu()
            
            # X√≥a reference ƒë·∫øn model
            del self.current_model
            self.current_model = None
            
            # Clear CUDA cache n·∫øu d√πng GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Force garbage collection ƒë·ªÉ gi·∫£i ph√≥ng RAM
            gc.collect()
            
            print("üóëÔ∏è ƒê√£ x√≥a model c≈© kh·ªèi RAM")
    
    def _clear_old_cache_files(self, keep_file: Optional[Path] = None):
        """
        X√≥a c√°c cache files c≈©, ch·ªâ gi·ªØ l·∫°i file hi·ªán t·∫°i
        Docker: Ch·ªâ x√≥a n·∫øu kh√¥ng d√πng persistent volume
        
        Args:
            keep_file: File cache c·∫ßn gi·ªØ l·∫°i
        """
        try:
            # Check n·∫øu ƒëang d√πng persistent volume (Docker)
            is_persistent = os.getenv("MODEL_CACHE_PERSISTENT", "false").lower() == "true"
            
            if is_persistent:
                print("üì¶ Persistent cache enabled, keeping old files")
                return
            
            for cache_file in self.model_cache_dir.glob("*.pt"):
                if keep_file is None or cache_file != keep_file:
                    cache_file.unlink()
                    print(f"üóëÔ∏è ƒê√£ x√≥a cache c≈©: {cache_file.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x√≥a cache: {e}")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """
        L·∫•y th√¥ng tin s·ª≠ d·ª•ng RAM v√† VRAM (Docker-aware)
        
        Returns:
            Dict: Th√¥ng tin memory usage (MB)
        """
        try:
            import psutil
            
            # RAM usage (Docker container memory)
            process = psutil.Process()
            ram_mb = process.memory_info().rss / 1024 / 1024
            
            # Docker memory limit
            try:
                with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:
                    docker_limit = int(f.read()) / 1024 / 1024
                    # Fix n·∫øu limit qu√° l·ªõn (no limit)
                    if docker_limit > 1000000:
                        docker_limit = None
            except:
                docker_limit = None
            
            memory_info = {
                "ram_usage_mb": round(ram_mb, 2),
                "docker_memory_limit_mb": round(docker_limit, 2) if docker_limit else None,
                "model_loaded": self.current_model is not None,
                "cache_dir": str(self.model_cache_dir),
                "is_docker": self._is_running_in_docker()
            }
            
            # VRAM usage n·∫øu c√≥ GPU
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
                gpu_cached = torch.cuda.memory_reserved() / 1024 / 1024
                memory_info.update({
                    "gpu_memory_allocated_mb": round(gpu_memory, 2),
                    "gpu_memory_cached_mb": round(gpu_cached, 2)
                })
            
            return memory_info
        except ImportError:
            # Fallback n·∫øu kh√¥ng c√≥ psutil
            return {
                "ram_usage_mb": 0,
                "model_loaded": self.current_model is not None,
                "is_docker": self._is_running_in_docker(),
                "cache_dir": str(self.model_cache_dir)
            }
    
    def _is_running_in_docker(self) -> bool:
        """
        Check n·∫øu ƒëang ch·∫°y trong Docker container
        """
        try:
            with open('/proc/1/cgroup', 'r') as f:
                return 'docker' in f.read()
        except:
            return os.path.exists('/.dockerenv')
    
    async def load_active_model(self) -> bool:
        """
        T·ª± ƒë·ªông load model ƒëang active t·ª´ database (Docker-friendly)
        """
        try:
            # T√¨m model ƒëang active
            active_model = await models_collection.find_one({"is_active": True})
            
            if not active_model:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y model n√†o ƒëang active")
                return False
            
            # Check n·∫øu model hi·ªán t·∫°i ƒë√£ l√† model n√†y r·ªìi
            if (self.model_info and 
                self.model_info.get("id") == active_model.get("id") and
                self.current_model is not None):
                print("‚ÑπÔ∏è Model n√†y ƒë√£ ƒë∆∞·ª£c load r·ªìi, kh√¥ng c·∫ßn load l·∫°i")
                return True
            
            print(f"üîÑ Loading model: {active_model['name']} v{active_model.get('version', 'unknown')}")
            print(f"üìä Memory tr∆∞·ªõc khi load: {self.get_memory_usage()}")
            
            # Load model t·ª´ URL
            success = await self.load_model_from_url(active_model['url'])
            
            if success:
                self.model_info = active_model
                print(f"‚úÖ Model loaded th√†nh c√¥ng: {active_model['name']}")
                print(f"üìä Memory sau khi load: {self.get_memory_usage()}")
                return True
            else:
                return False
                
        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            return False
    
    async def load_model_from_url(self, model_url: str) -> bool:
        """
        Load PyTorch model t·ª´ S3 URL v·ªõi Docker support
        
        Args:
            model_url: URL c·ªßa model checkpoint tr√™n S3
            
        Returns:
            bool: True n·∫øu load th√†nh c√¥ng
        """
        try:
            # X√≥a model c≈© kh·ªèi RAM tr∆∞·ªõc
            self._clear_old_model_from_memory()
            
            # T·∫°o t√™n file cache
            cache_filename = f"model_{hash(model_url)}.pt"
            cache_path = self.model_cache_dir / cache_filename
            
            # Check cache exists (c√≥ th·ªÉ persistent trong Docker volume)
            if cache_path.exists():
                print(f"üìÅ S·ª≠ d·ª•ng cache: {cache_path.name}")
            else:
                # Download model t·ª´ S3
                print(f"üì• Downloading model t·ª´ S3...")
                success = await self._download_model(model_url, cache_path)
                if not success:
                    return False
            
            # X√≥a c√°c cache files c≈© (n·∫øu kh√¥ng persistent)
            self._clear_old_cache_files(keep_file=cache_path)
            self.current_cache_file = cache_path
            
            # Load PyTorch checkpoint
            print(f"üß† Loading model v√†o RAM...")
            success = self._load_model_from_file(cache_path)
            
            if success:
                print(f"‚úÖ Model loaded v√†o RAM th√†nh c√¥ng!")
                return True
            else:
                return False
            
        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            # Cleanup n·∫øu l·ªói
            self._clear_old_model_from_memory()
            return False
    
    async def _download_model(self, model_url: str, cache_path: Path) -> bool:
        """
        Download model t·ª´ S3 v·ªõi retry v√† timeout cho Docker
        """
        max_retries = 3
        timeout = 300  # 5 minutes timeout
        
        for attempt in range(max_retries):
            try:
                connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300)
                timeout_config = aiohttp.ClientTimeout(total=timeout)
                
                async with aiohttp.ClientSession(
                    connector=connector, 
                    timeout=timeout_config
                ) as session:
                    async with session.get(model_url) as response:
                        if response.status == 200:
                            with open(cache_path, 'wb') as f:
                                async for chunk in response.content.iter_chunked(8192):
                                    f.write(chunk)
                            print(f"‚úÖ Downloaded: {cache_path.name}")
                            return True
                        else:
                            print(f"‚ùå HTTP {response.status}")
                            
            except asyncio.TimeoutError:
                print(f"‚è∞ Timeout attempt {attempt + 1}/{max_retries}")
            except Exception as e:
                print(f"‚ùå Download error attempt {attempt + 1}/{max_retries}: {e}")
            
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
        
        return False
    
    def _load_model_from_file(self, cache_path: Path) -> bool:
        """
        Load model t·ª´ file v·ªõi error handling cho Docker
        """
        try:
            # Load PyTorch checkpoint
            checkpoint = torch.load(cache_path, map_location=self.device)
            
            # T·∫°o model architecture (gi·∫£ s·ª≠ InceptionV3)
            from torchvision.models import inception_v3
            model = inception_v3(pretrained=False, num_classes=6)
            
            # Load state dict
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            
            # Handle DataParallel prefix
            if list(state_dict.keys())[0].startswith('module.'):
                from collections import OrderedDict
                new_state_dict = OrderedDict()
                for k, v in state_dict.items():
                    name = k[7:]  # Remove 'module.' prefix
                    new_state_dict[name] = v
                model.load_state_dict(new_state_dict)
            else:
                model.load_state_dict(state_dict)
            
            # Move model to device v√† set eval mode
            model = model.to(self.device)
            model.eval()
            
            # Clear checkpoint t·ª´ memory ƒë·ªÉ ti·∫øt ki·ªám RAM
            del checkpoint
            del state_dict
            gc.collect()
            
            self.current_model = model
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói load model t·ª´ file: {e}")
            return False
    
    def predict_image(self, image_url: str) -> Tuple[Optional[int], Optional[str], Optional[float], Optional[List[float]]]:
        """
        D·ª± ƒëo√°n ·∫£nh t·ª´ URL (Docker-optimized)
        
        Args:
            image_url: URL c·ªßa ·∫£nh c·∫ßn d·ª± ƒëo√°n
            
        Returns:
            Tuple: (predicted_label_id, predicted_class_name, predicted_probability, all_probabilities)
        """
        if self.current_model is None:
            print("‚ö†Ô∏è Model ch∆∞a ƒë∆∞·ª£c load")
            return None, None, None, None
        
        try:
            # Download ·∫£nh t·ª´ URL v·ªõi timeout
            timeout = 30  # 30 seconds timeout cho Docker
            response = requests.get(image_url, stream=True, timeout=timeout)
            if response.status_code != 200:
                print(f"‚ùå HTTP {response.status_code} khi download ·∫£nh")
                return None, None, None, None
            
            # M·ªü ·∫£nh b·∫±ng PIL
            img = Image.open(response.raw)
            image = img.convert('RGB')
            
            # Apply transform
            image = self.transform(image)
            image = image.unsqueeze(0)  # Add batch dimension
            image = image.to(self.device)
            
            # Predict
            self.current_model.eval()
            with torch.no_grad():
                outputs = self.current_model(image)
                
                # Handle InceptionV3 auxiliary output
                if isinstance(outputs, tuple):
                    outputs = outputs[0]
            
            # Get probabilities
            probabilities = torch.softmax(outputs, dim=1)
            predicted_probability, predicted_label_id = torch.max(probabilities, 1)
            
            # Get class name
            predicted_class_name = self.class_names[predicted_label_id.item()]
            
            # Convert all probabilities to list
            all_probabilities = probabilities[0].cpu().numpy().tolist()
            
            # Cleanup tensors ƒë·ªÉ ti·∫øt ki·ªám memory
            del image, outputs, probabilities
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return (
                predicted_label_id.item(),
                predicted_class_name,
                predicted_probability.item(),
                all_probabilities
            )
            
        except requests.exceptions.Timeout:
            print("‚è∞ Timeout khi download ·∫£nh")
            return None, None, None, None
        except Exception as e:
            print(f"‚ùå L·ªói predict: {e}")
            return None, None, None, None
    
    def predict(self, image_url: str) -> List[float]:
        """
        Predict v√† tr·∫£ v·ªÅ m·∫£ng x√°c su·∫•t cho 6 l·ªõp
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            
        Returns:
            List[float]: M·∫£ng x√°c su·∫•t 6 ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng v·ªõi l·ªõp 0-5
        """
        _, _, _, all_probabilities = self.predict_image(image_url)
        
        if all_probabilities is None:
            # Fallback: equal probability
            return [0.16, 0.17, 0.17, 0.17, 0.16, 0.17]
        
        return all_probabilities
    
    def get_prediction_details(self, image_url: str) -> Dict:
        """
        L·∫•y th√¥ng tin d·ª± ƒëo√°n chi ti·∫øt
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            
        Returns:
            Dict: Th√¥ng tin d·ª± ƒëo√°n chi ti·∫øt
        """
        predicted_id, predicted_class, predicted_prob, all_probs = self.predict_image(image_url)
        
        if predicted_id is None:
            return {
                "success": False,
                "error": "Kh√¥ng th·ªÉ d·ª± ƒëo√°n ·∫£nh",
                "memory_usage": self.get_memory_usage()
            }
        
        # Map class names to BI-RADS
        bi_rads_names = [
            "BI-RADS 0 - C·∫ßn ƒë√°nh gi√° th√™m",
            "BI-RADS 1 - √Çm t√≠nh", 
            "BI-RADS 2 - T·ªïn th∆∞∆°ng l√†nh t√≠nh",
            "BI-RADS 3 - C√≥ th·ªÉ l√†nh t√≠nh",
            "BI-RADS 4 - Nghi ng·ªù √°c t√≠nh",
            "BI-RADS 5 - R·∫•t nghi ng·ªù √°c t√≠nh"
        ]
        
        # T·∫°o probability distribution
        probability_distribution = []
        for i, prob in enumerate(all_probs):
            probability_distribution.append({
                "class_id": i,
                "class_name": bi_rads_names[i],
                "probability": prob,
                "percentage": prob * 100
            })
        
        # X√°c ƒë·ªãnh risk level
        if predicted_id <= 2:
            risk_level = "Low"
            recommendation = "Ti·∫øp t·ª•c t·∫ßm so√°t ƒë·ªãnh k·ª≥ theo l·ªãch"
        elif predicted_id == 3:
            risk_level = "Medium" 
            recommendation = "Theo d√µi ng·∫Øn h·∫°n trong 6 th√°ng"
        else:
            risk_level = "High"
            recommendation = "C·∫ßn sinh thi·∫øt v√† thƒÉm kh√°m chuy√™n s√¢u ngay"
        
        return {
            "success": True,
            "predicted_class_id": predicted_id,
            "predicted_class_name": bi_rads_names[predicted_id],
            "confidence": predicted_prob,
            "probability_distribution": probability_distribution,
            "risk_level": risk_level,
            "recommendation": recommendation,
            "model_info": self.get_model_info(),
            "memory_usage": self.get_memory_usage()
        }
    
    def get_model_info(self) -> Optional[Dict]:
        """
        L·∫•y th√¥ng tin model hi·ªán t·∫°i
        """
        if self.model_info:
            return {
                "id": self.model_info.get("id"),
                "name": self.model_info.get("name"),
                "version": self.model_info.get("version"),
                "url": self.model_info.get("url"),
                "is_active": self.model_info.get("is_active"),
                "loaded": self.current_model is not None,
                "device": str(self.device),
                "cache_file": str(self.current_cache_file) if self.current_cache_file else None
            }
        return None
    
    def is_model_loaded(self) -> bool:
        """
        Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a
        """
        return self.current_model is not None
    
    async def reload_active_model(self) -> bool:
        """
        Reload model active m·ªõi nh·∫•t t·ª´ database
        """
        print("üîÑ Reloading active model...")
        return await self.load_active_model()
    
    def clear_all_memory(self):
        """
        X√≥a ho√†n to√†n model v√† cache ƒë·ªÉ gi·∫£i ph√≥ng memory
        """
        print("üßπ Clearing all memory and cache...")
        self._clear_old_model_from_memory()
        self._clear_old_cache_files()
        self.model_info = None
        self.current_cache_file = None
        print("‚úÖ Memory cleared!")


# Singleton instance
model_ai = ModelAI()


# H√†m test
async def test_model_ai():
    """
    Test ModelAI v·ªõi ·∫£nh sample (Docker-friendly)
    """
    print("üß™ Testing ModelAI...")
    print(f"üìä Memory ban ƒë·∫ßu: {model_ai.get_memory_usage()}")
    
    # Load active model
    success = await model_ai.load_active_model()
    
    if success:
        print("‚úÖ Model loaded successfully")
        print(f"üìä Memory sau load: {model_ai.get_memory_usage()}")
        
        # Test v·ªõi sample image URL
        sample_url = "https://example.com/sample_mammogram.jpg"
        
        # Test predict
        probabilities = model_ai.predict(sample_url)
        print(f"üìä Probabilities: {[f'{p:.3f}' for p in probabilities]}")
        
        # Test detailed prediction
        details = model_ai.get_prediction_details(sample_url)
        print(f"üìã Prediction details: {details}")
        
    else:
        print("‚ùå No active model found")


if __name__ == "__main__":
    asyncio.run(test_model_ai())
